#!/usr/bin/env python3
"""
AWS Inspector Multi-Repository Vulnerability Analyzer - MVP
Simple tool to highlight common CVEs across repositories
"""

import streamlit as st
import json
import pandas as pd
from collections import defaultdict
from datetime import datetime

st.set_page_config(page_title="Vuln Analyzer", layout="wide")

# Initialize session state for annotations
if 'annotations' not in st.session_state:
    st.session_state.annotations = {}

def get_annotation_key(cve_id, repo_name):
    """Create unique key for CVE/repo combination"""
    return f"{cve_id}_{repo_name}"

def save_annotation(cve_id, repo_name, exploitable, notes):
    """Save annotation for a CVE/repo combination"""
    key = get_annotation_key(cve_id, repo_name)
    st.session_state.annotations[key] = {
        'exploitable': exploitable,
        'notes': notes,
        'updated': datetime.now().isoformat()
    }

def get_annotation(cve_id, repo_name):
    """Get annotation for a CVE/repo combination"""
    key = get_annotation_key(cve_id, repo_name)
    return st.session_state.annotations.get(key, {
        'exploitable': 'Unknown',
        'notes': '',
        'updated': None
    })

def parse_json(data):
    """Parse AWS Inspector JSON with nested structure"""
    raw_findings = []
    
    if isinstance(data, list):
        raw_findings = data
    elif isinstance(data, dict) and 'findings' in data:
        raw_findings = data['findings']
    else:
        raw_findings = [data]
    
    # Flatten the nested structure
    flattened = []
    
    for finding in raw_findings:
        # Extract vulnerability details
        pkg_details = finding.get('packageVulnerabilityDetails', {})
        vuln_id = pkg_details.get('vulnerabilityId', 'UNKNOWN')
        vuln_packages = pkg_details.get('vulnerablePackages', [])
        source_url = pkg_details.get('sourceUrl', finding.get('sourceUrl', ''))
        
        # Extract repository and image info from resources
        resources = finding.get('resources', [])
        repo_name = 'unknown'
        image_tags = []
        aws_account = 'unknown'
        
        for resource in resources:
            # Get ECR details
            details = resource.get('details', {})
            ecr = details.get('awsEcrContainerImage', {})
            
            if ecr:
                repo_name = ecr.get('repositoryName', repo_name)
                aws_account = ecr.get('registry', aws_account)  # Get account from registry field
                tags = ecr.get('imageTags', [])
                if tags:
                    image_tags.extend(tags)
        
        if not image_tags:
            image_tags = ['latest']
        
        # Get severity and description from finding level
        severity = finding.get('severity', 'UNKNOWN')
        description = finding.get('description', '')
        
        # Create flattened finding
        for pkg in (vuln_packages if vuln_packages else [{}]):
            for tag in image_tags:
                flattened.append({
                    'vulnerabilityId': vuln_id,
                    'repositoryName': repo_name,
                    'imageTag': tag,
                    'awsAccountId': aws_account,
                    'severity': severity,
                    'sourceUrl': source_url,
                    'description': description,
                    'packageName': pkg.get('name', 'unknown'),
                    'packageVersion': pkg.get('version', 'unknown'),
                    'fixedInVersion': pkg.get('fixedInVersion', 'N/A')
                })
    
    return flattened

def find_common_cves(findings):
    """Find CVEs in multiple repos"""
    cve_repos = defaultdict(set)
    
    for f in findings:
        cve_repos[f['vulnerabilityId']].add(f['repositoryName'])
    
    return {cve: repos for cve, repos in cve_repos.items() if len(repos) > 1}

st.title("ðŸ” Vulnerability Analyzer")

uploaded_files = st.file_uploader("Upload JSON files", type=['json'], accept_multiple_files=True)

# Add master Excel uploader
st.markdown("---")
master_file = st.file_uploader(
    "ðŸ“Š Upload Master Excel File (optional - to load previous annotations)",
    type=['xlsx', 'xls'],
    help="Upload your master Excel file with previous annotations. Each sheet should represent one image/repository."
)

# Load master Excel if provided
if master_file:
    try:
        # Read all sheets from the Excel file
        excel_file = pd.ExcelFile(master_file)
        sheet_names = excel_file.sheet_names
        
        st.info(f"ðŸ“Š Master Excel loaded: {len(sheet_names)} sheets found ({', '.join(sheet_names[:3])}{'...' if len(sheet_names) > 3 else ''})")
        
        # Load annotations from all sheets
        loaded_count = 0
        total_records = 0
        
        for sheet_name in sheet_names:
            sheet_df = pd.read_excel(excel_file, sheet_name=sheet_name)
            total_records += len(sheet_df)
            
            for _, row in sheet_df.iterrows():
                cve_id = str(row['CVE'])
                repo = str(row.get('Repository', sheet_name))  # Use sheet name as fallback
                
                # Only load if there are notes (meaning it was reviewed)
                notes = str(row.get('Notes', ''))
                exploitable = str(row.get('Exploitable', 'Unknown'))
                
                if notes and notes != 'nan' and notes.strip():
                    key = get_annotation_key(cve_id, repo)
                    # Support both "First Seen" and "Last Updated" column names
                    timestamp = str(row.get('First Seen', row.get('Last Updated', '')))
                    st.session_state.annotations[key] = {
                        'exploitable': exploitable,
                        'notes': notes,
                        'updated': timestamp
                    }
                    loaded_count += 1
        
        st.success(f"âœ… Loaded {loaded_count} annotations from {len(sheet_names)} sheets ({total_records} total records)")
        
    except Exception as e:
        st.error(f"Error loading master Excel file: {str(e)}")
        import traceback
        st.code(traceback.format_exc())


if uploaded_files and len(uploaded_files) > 0:
    all_findings = []
    
    # Parse all uploaded files at once
    with st.spinner(f'Processing {len(uploaded_files)} files...'):
        for uploaded in uploaded_files:
            try:
                data = json.load(uploaded)
                findings = parse_json(data)
                all_findings.extend(findings)
            except Exception as e:
                st.error(f"Error processing {uploaded.name}: {str(e)}")
    
    if len(all_findings) == 0:
        st.warning("No findings found in uploaded files")
    else:
        # Filter to only NEW findings (those without notes)
        new_findings = []
        for f in all_findings:
            cve_id = f['vulnerabilityId']
            repo = f['repositoryName']
            annotation = get_annotation(cve_id, repo)
            
            # Only include if no notes exist (new finding)
            if not annotation['notes'] or annotation['notes'].strip() == '':
                new_findings.append(f)
        
        st.success(f"âœ… Loaded {len(all_findings)} total findings from {len(uploaded_files)} files. There are {len(new_findings)} new findings for review.")
        
        # Use new_findings for display instead of all_findings
        display_findings = new_findings if new_findings else all_findings
        
        # Show info if we're filtering
        if len(new_findings) < len(all_findings):
            reviewed_count = len(all_findings) - len(new_findings)
            st.info(f"â„¹ï¸ Displaying only new findings. {reviewed_count} previously reviewed findings are hidden but will be included in exports.")
        elif len(new_findings) == 0:
            st.warning("âš ï¸ No new findings! All items have been previously reviewed. Showing all findings for reference.")

        
        # Debug: show first finding from each file
        with st.expander("ðŸ” Debug: First finding from each parsed result"):
            # Show sample findings
            seen_repos = set()
            for f in display_findings[:10]:  # Show first 10
                repo = f['repositoryName']
                if repo not in seen_repos:
                    st.write(f"**Repository:** {repo}")
                    st.json(f)
                    seen_repos.add(repo)
                    st.markdown("---")
        
        repos = sorted(set(f['repositoryName'] for f in display_findings))
        common_cves = find_common_cves(display_findings)
        
        # Debug repository detection
        with st.expander("ðŸ” Debug: Repositories found"):
            st.write(f"Total unique repositories: {len(repos)}")
            st.write(f"Repository list: {repos}")
            for repo in repos:
                count = len([f for f in display_findings if f['repositoryName'] == repo])
                st.write(f"  â€¢ `{repo}`: {count} findings")
        
        st.info(f"ðŸ“ {len(repos)} repositories | ðŸ”— {len(common_cves)} common CVEs")
        
        tab1, tab2, tab3 = st.tabs(["Common CVEs", "By Repository", "Export"])
    
    with tab1:
        st.header("Common CVEs")
        st.markdown("**CVEs affecting multiple repositories**")
        
        if not common_cves:
            st.info("No common CVEs found")
        else:
            for cve, repos in sorted(common_cves.items(), key=lambda x: -len(x[1])):
                # Get details from first finding
                details = next(f for f in display_findings if f['vulnerabilityId'] == cve)
                severity = details['severity']
                
                color = 'ðŸ”´' if severity == 'CRITICAL' else 'ðŸŸ ' if severity == 'HIGH' else 'ðŸŸ¡'
                
                with st.expander(f"{color} **{cve}** [{severity}] - {len(repos)} repos"):
                    st.write(f"**Package:** {details['packageName']}")
                    st.write(f"**Affected repositories:**")
                    for repo in sorted(repos):
                        st.write(f"  â€¢ {repo}")
                    
                    if details['description']:
                        with st.expander("Description"):
                            st.write(details['description'])
    
    with tab2:
        st.header("By Repository")
        
        selected = st.selectbox("Repository", repos)
        repo_findings = [f for f in display_findings if f['repositoryName'] == selected]
        
        st.write(f"**{len(repo_findings)} findings**")
        
        # Group by CVE to avoid duplicates
        cve_map = {}
        for f in repo_findings:
            cve_id = f['vulnerabilityId']
            if cve_id not in cve_map:
                cve_map[cve_id] = f
        
        for cve_id, f in sorted(cve_map.items()):
            severity = f['severity']
            is_common = cve_id in common_cves
            
            color = 'ðŸ”´' if severity == 'CRITICAL' else 'ðŸŸ ' if severity == 'HIGH' else 'ðŸŸ¡'
            badge = " ðŸ”— COMMON" if is_common else ""
            
            # Get existing annotation
            annotation = get_annotation(cve_id, selected)
            exploit_badge = ""
            if annotation['exploitable'] == 'Yes':
                exploit_badge = " âš ï¸ EXPLOITABLE"
            elif annotation['exploitable'] == 'No':
                exploit_badge = " âœ… NOT EXPLOITABLE"
            
            with st.expander(f"{color} {cve_id} [{severity}]{badge}{exploit_badge}"):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write(f"**CVE:** {cve_id}")
                    st.write(f"**Package:** {f['packageName']}")
                    st.write(f"**Version:** {f['packageVersion']}")
                    st.write(f"**Fixed in:** {f['fixedInVersion']}")
                
                with col2:
                    st.write(f"**Severity:** {severity}")
                    st.write(f"**Image Tag:** {f['imageTag']}")
                    st.write(f"**AWS Account:** {f['awsAccountId']}")
                    if f['sourceUrl']:
                        st.markdown(f"[ðŸ”— Source]({f['sourceUrl']})")
                
                if is_common:
                    other_repos = [r for r in common_cves[cve_id] if r != selected]
                    if other_repos:
                        st.info(f"âš ï¸ Also found in: {', '.join(other_repos)}")
                
                if f['description']:
                    with st.expander("Description"):
                        st.write(f['description'])
                
                # Annotation Section
                st.markdown("---")
                st.subheader("ðŸ“ Annotation")
                
                with st.form(key=f"annotation_{cve_id}_{selected}"):
                    exploitable = st.selectbox(
                        "Exploitable in this application?",
                        options=['Unknown', 'Yes', 'No', 'N/A - False Positive'],
                        index=['Unknown', 'Yes', 'No', 'N/A - False Positive'].index(annotation['exploitable'])
                    )
                    
                    notes = st.text_area(
                        "Research Notes",
                        value=annotation['notes'],
                        placeholder="Add notes about your research, testing results, remediation plans, etc.",
                        height=100
                    )
                    
                    # Option to apply to all repos
                    apply_all = False
                    if is_common:
                        apply_all = st.checkbox(
                            f"Apply to all {len(common_cves[cve_id])} repositories with this CVE",
                            help="Use this if the exploitability is the same across all repos (e.g., shared base image issue)"
                        )
                    
                    col1, col2 = st.columns([1, 3])
                    with col1:
                        if st.form_submit_button("ðŸ’¾ Save"):
                            if apply_all and is_common:
                                # Apply to all repos
                                for repo_name in common_cves[cve_id]:
                                    save_annotation(cve_id, repo_name, exploitable, notes)
                                st.success(f"Saved to all {len(common_cves[cve_id])} repositories!")
                            else:
                                # Apply to current repo only
                                save_annotation(cve_id, selected, exploitable, notes)
                                st.success("Saved!")
                            st.rerun()
                    
                    with col2:
                        if annotation['updated']:
                            st.caption(f"Last updated: {annotation['updated'][:19]}")
                
                # Show existing annotation if present
                if annotation['notes']:
                    st.info(f"**Current Status:** {annotation['exploitable']}")
                    st.write(f"**Notes:** {annotation['notes']}")
    
    with tab3:
        st.header("Export")
        st.markdown("**Exports ALL findings (both new and previously reviewed)**")
        st.markdown("*Each repository will be exported as a separate sheet in the Excel file*")
        
        # Create DataFrame with annotations
        export_data = []
        for f in all_findings:
            cve_id = f['vulnerabilityId']
            repo = f['repositoryName']
            annotation = get_annotation(cve_id, repo)
            
            # Combine package and version like "pam:1.3.1"
            package_version = f"{f['packageName']}:{f['packageVersion']}"
            
            export_data.append({
                'CVE': cve_id,
                'Severity': f['severity'],
                'Repository': repo,
                'Image Tag': f['imageTag'],
                'First Seen': annotation['updated'] or '',
                'Package': package_version,  # Combined format
                'AWS Account': f['awsAccountId'],
                'Exploitable': annotation['exploitable'],
                'Notes': annotation['notes']                
            })
        
        df = pd.DataFrame(export_data)
        
        # Remove duplicate CVE/Repo combinations
        df = df.drop_duplicates(subset=['CVE', 'Repository'])
        
        st.dataframe(df, use_container_width=True)
        
        # Export options
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ðŸ“¥ CSV Export")
            csv = df.to_csv(index=False)
            st.download_button(
                "Download CSV (Single File)",
                csv,
                f"vulnerabilities_annotated_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                "text/csv",
                help="Single CSV file with all repositories"
            )
        
        with col2:
            st.subheader("ðŸ“Š Excel Export")
            st.markdown("*Recommended: One sheet per repository*")
            
            # Create Excel file with one sheet per repository
            from io import BytesIO
            output = BytesIO()
            
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                # Get unique repositories
                unique_repos = df['Repository'].unique()
                
                for repo in unique_repos:
                    # Filter data for this repository
                    repo_df = df[df['Repository'] == repo].copy()
                    
                    # Clean sheet name (Excel has limitations)
                    sheet_name = repo[:31]  # Excel sheet name limit is 31 chars
                    # Remove invalid characters
                    for char in ['\\', '/', '*', '[', ']', ':', '?']:
                        sheet_name = sheet_name.replace(char, '_')
                    
                    # Write to sheet
                    repo_df.to_excel(writer, sheet_name=sheet_name, index=False)
                    
                    # Auto-adjust column widths
                    worksheet = writer.sheets[sheet_name]
                    for idx, col in enumerate(repo_df.columns):
                        max_length = max(
                            repo_df[col].astype(str).apply(len).max(),
                            len(col)
                        )
                        worksheet.column_dimensions[chr(65 + idx)].width = min(max_length + 2, 50)
            
            excel_data = output.getvalue()
            
            st.download_button(
                "Download Excel (One Sheet Per Repo)",
                excel_data,
                f"Security_Assessment_Review_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                help=f"Excel file with {len(unique_repos)} sheets (one per repository)"
            )
            
            st.caption(f"Will create {len(unique_repos)} sheets: {', '.join(list(unique_repos)[:3])}{'...' if len(unique_repos) > 3 else ''}")
        
        # Summary of annotations
        st.markdown("---")
        st.subheader("Annotation Summary")
        
        total_annotated = sum(1 for a in st.session_state.annotations.values() if a['notes'])
        exploitable_count = sum(1 for a in st.session_state.annotations.values() if a['exploitable'] == 'Yes')
        not_exploitable_count = sum(1 for a in st.session_state.annotations.values() if a['exploitable'] == 'No')
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Annotated", total_annotated)
        with col2:
            st.metric("Exploitable", exploitable_count)
        with col3:
            st.metric("Not Exploitable", not_exploitable_count)